version: '3'

networks:
  default:
    name: ollama-network
    external: true

services:
  lusha_llama3b_wrongs:
    environment:
      - OLLAMA_URL=http://lusha_ollama:11434
      - PYTHONUNBUFFERED=1
      - PYTHONFAULTHANDLER=1
      - PYTHONIOENCODING=UTF-8
    stdin_open: true  # Keep stdin open
    tty: true        # Allocate a pseudo-TTY
    build: .
    container_name: req_gen
    user: "${UID}:${GID}"
    
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              count: 0
    volumes:
      - /home/lusha/storage/star_code/outputs:/app/outputs
      - /home/lusha/storage/star_code/data:/app/data
      - /home/lusha/datasets/private:/app/data/datasets
      - /home/lusha/storage/star_code/src:/app/src
    cpuset: "12-15"
    networks:
      - default
