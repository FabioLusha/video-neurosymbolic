{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "random.seed(6)\n",
    "np.random.seed(6)\n",
    "\n",
    "WORK_DIR = Path.cwd().parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "# Run if needed\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_map(label_dir):\n",
    "    vocab_map = dict()\n",
    "\n",
    "    with open(label_dir) as in_file:\n",
    "        for line in in_file.readlines():\n",
    "            mapping = line.strip('\\n')\n",
    "            key, val = mapping.split(' ')\n",
    "            vocab_map[key] = val\n",
    "    \n",
    "    return vocab_map\n",
    "\n",
    "def textual_stsg(q_data):\n",
    "    frame_ids = sorted(q_data['situations'].keys())\n",
    "\n",
    "    stsg = {}\n",
    "    for f in frame_ids:\n",
    "        frame_sg = []\n",
    "        rels = q_data['situations'][f]['rel_labels']\n",
    "        \n",
    "        if rels == []:\n",
    "            # There are some frames in the ground truth without any scene graph\n",
    "            # annotation\n",
    "            continue\n",
    "        for rel_pair, rel in zip(q_data['situations'][f]['rel_pairs'], rels):\n",
    "            obj1, obj2 = rel_pair\n",
    "            frame_sg.append([obj_vocab[obj1], rel_vocab[rel], obj_vocab[obj2]])\n",
    "\n",
    "        stsg[f] = frame_sg\n",
    "    \n",
    "    return stsg\n",
    "\n",
    "\n",
    "obj_vocab = get_vocab_map(WORK_DIR / 'data/datasets/STAR/STAR_annotations/class_maps/object_classes.txt')\n",
    "rel_vocab = get_vocab_map(WORK_DIR /  'data/datasets/STAR/STAR_annotations/class_maps/relationship_classes.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o000': 'person',\n",
       " 'o001': 'broom',\n",
       " 'o002': 'picture',\n",
       " 'o003': 'closet/cabinet',\n",
       " 'o004': 'blanket',\n",
       " 'o005': 'window',\n",
       " 'o006': 'table',\n",
       " 'o007': 'paper/notebook',\n",
       " 'o008': 'refrigerator',\n",
       " 'o009': 'pillow',\n",
       " 'o010': 'cup/glass/bottle',\n",
       " 'o011': 'shelf',\n",
       " 'o012': 'shoe',\n",
       " 'o013': 'medicine',\n",
       " 'o014': 'phone/camera',\n",
       " 'o015': 'box',\n",
       " 'o016': 'sandwich',\n",
       " 'o017': 'book',\n",
       " 'o018': 'bed',\n",
       " 'o019': 'clothes',\n",
       " 'o020': 'mirror',\n",
       " 'o021': 'sofa/couch',\n",
       " 'o022': 'floor',\n",
       " 'o023': 'bag',\n",
       " 'o024': 'dish',\n",
       " 'o025': 'laptop',\n",
       " 'o026': 'door',\n",
       " 'o027': 'towel',\n",
       " 'o028': 'food',\n",
       " 'o029': 'chair',\n",
       " 'o030': 'doorknob',\n",
       " 'o031': 'doorway',\n",
       " 'o032': 'groceries',\n",
       " 'o033': 'hands',\n",
       " 'o034': 'light',\n",
       " 'o035': 'vacuum',\n",
       " 'o036': 'television'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r000': 'on',\n",
       " 'r001': 'behind',\n",
       " 'r002': 'in_front_of',\n",
       " 'r003': 'on_the_side_of',\n",
       " 'r004': 'above',\n",
       " 'r005': 'beneath',\n",
       " 'r006': 'drinking_from',\n",
       " 'r007': 'have_it_on_the_back',\n",
       " 'r008': 'wearing',\n",
       " 'r009': 'holding',\n",
       " 'r010': 'lying_on',\n",
       " 'r011': 'covered_by',\n",
       " 'r012': 'carrying',\n",
       " 'r013': 'eating',\n",
       " 'r014': 'leaning_on',\n",
       " 'r015': 'sitting_on',\n",
       " 'r016': 'twisting',\n",
       " 'r017': 'writing_on',\n",
       " 'r018': 'standing_on',\n",
       " 'r019': 'touching',\n",
       " 'r020': 'wiping',\n",
       " 'r021': 'at',\n",
       " 'r022': 'under',\n",
       " 'r023': 'near'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordent\n",
    "Let's use `wordent` an try to aggregate words with simialry meaning.\n",
    "\n",
    "\n",
    "> WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. \n",
    "> \n",
    "> WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However, there are some important distinctions. \n",
    "> - First, WordNet interlinks not just word forms—strings of letters—but specific senses of words. As a result, words that are found in close proximity to one another in the network are semantically disambiguated. \n",
    "> - Second, WordNet labels the semantic relations among words, whereas the groupings of words in a thesaurus does not follow any explicit pattern other than meaning similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('man.n.01'),\n",
       " Synset('serviceman.n.01'),\n",
       " Synset('man.n.03'),\n",
       " Synset('homo.n.02'),\n",
       " Synset('man.n.05'),\n",
       " Synset('man.n.06'),\n",
       " Synset('valet.n.01'),\n",
       " Synset('man.n.08'),\n",
       " Synset('man.n.09'),\n",
       " Synset('man.n.10'),\n",
       " Synset('world.n.08')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the different synsets (concept) that can be associated to\n",
    "# the word in the specified part-of-speech (pos)\n",
    "# \n",
    "# Synset: a set of synonyms that share a common meaning.\n",
    "wn.synsets('man', pos=wn.NOUN)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-llm",
   "language": "python",
   "name": "video-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
