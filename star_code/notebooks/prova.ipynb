{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib  \n",
    "import os, sys\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import ollama_manager as om\n",
    "\n",
    "import STAR_utils.visualization_tools.qa_visualization as qaviz\n",
    "import main\n",
    "\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please replace following data path to your local path\n",
    "# all data can be download from our homepages (http://star.csail.mit.edu or https://bobbywu.com/STAR)\n",
    "\n",
    "# root_dir = '/root/user/'\n",
    "raw_video_dir = pathlib.Path('../data/datasets/action-genome/Charades_v1_480/')\n",
    "raw_frame_dir = pathlib.Path('../data/datasets/action-genome/frames/')\n",
    "annotation_dir = pathlib.Path('STAR_utils/annotations/STAR_classes/')\n",
    "# pose_dir = root_dir + 'STAR/pose/'\n",
    "# qa_train_dir = root_dir + 'STAR_train.json' \n",
    "# qa_val_dir = root_dir + 'STAR/STAR_val.json' \n",
    "# qa_test_dir = root_dir + 'STAR/STAR_test.json' \n",
    "save_video_dir = pathlib.Path('/home/lusha/visualization_tmp')\n",
    "fps = pickle.load(open(annotation_dir / 'video_fps','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import batch_processor\n",
    "\n",
    "importlib.reload(main)\n",
    "importlib.reload(batch_processor)\n",
    "\n",
    "SEED = 39\n",
    "url = os.environ.get(\"OLLAMA_URL\", \"http://lusha_ollama:11435\")\n",
    "\n",
    "sys_file_path = '../data/prompts/img_captioning/system_prompt.txt'\n",
    "sys_prompt = main._load_prompt_fromfile(sys_file_path)\n",
    "ollama_params={\n",
    "            \"model\": \"llama3.2\",\n",
    "            \"system\": sys_prompt,\n",
    "            \"stream\": True,\n",
    "            \"options\": {\n",
    "                \"num_ctx\": 10240,\n",
    "                \"temperature\": 0.1,\n",
    "                \"num_predict\": 512,\n",
    "                \"seed\": SEED,\n",
    "            },\n",
    "        }\n",
    "\n",
    "client = om.OllamaRequestManager(url, ollama_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '../outputs/gen_frames.jsonl'\n",
    "main.streaming_frame_generation(client, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interaction_T1_13'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../outputs/responses_genframes:7b_202504010_22:47:00.jsonl', 'r') as f:\n",
    "    content = [json.loads(l) for l in f.readlines()]\n",
    "    \n",
    "\n",
    "content[0]['qid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['qid', 'chat_history', 'stsg'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['role', 'content', 'images'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]['chat_history'][0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['content', 'role'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]['chat_history'][1].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['role', 'content'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]['chat_history'][2].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['role', 'content'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]['chat_history'][3].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrame 000198:\\n\\nyoung_man ---- standing_in_front_of ---- sofa\\nsofa ---- sitting_on ---- cushions\\ncushions ---- on ---- sofa\\nsofa ---- positioned_infront_of ---- wall\\nwall ---- plain_color ---- white\\nwall ---- mounted_with ---- electrical_outlet\\nelectrical_outlet ---- attached_to ---- wall\\nwall ---- mounted_with ---- light_switch\\nlight_switch ---- attached_to ---- wall\\nwall ---- background_for ---- sofa\\nwall ---- positioned_behind ---- young_man\\nyoung_man ---- looking_at ---- sofa\\nsofa ---- positioned_on ---- floor\\nfloor ---- covered_with ---- tiles\\ntiles ---- arranged_in_squares ---- floor\\n\\nFrame 000212:\\n\\nman ---- holding ---- backpack\\nman ---- standing_in_front_of ---- sofa\\nsofa ---- positioned_on ---- floor\\nsofa ---- containing ---- cushions\\ncushion ---- on ---- sofa\\ncushion ---- part_of ---- sofa\\nbackpack ---- held_by ---- man\\nsofa ---- adjacent_to ---- wall\\nwall ---- behind ---- man\\nelectrical_outlet ---- attached_to ---- wall\\nlight_fixture ---- attached_to ---- electrical_outlet\\nelectrical_outlet ---- on ---- wall\\nman ---- facing ---- sofa\\nsofa ---- occupying ---- space\\nwall ---- supporting ---- light_fixture\\nman ---- looking_at ---- sofa\\nfloor ---- supporting ---- sofa\\nsofa ---- positioned_above ---- floor\\n\\nFrame 000223:\\n\\nman ---- standing_in_front_of ---- sofa\\nsofa ---- has_on ---- throw_pillow\\nthrow_pillow ---- located_on ---- sofa\\nsofa ---- located_on ---- floor\\nfloor ---- made_of ---- tile\\ntile ---- arranged_in ---- pattern\\nwall ---- located_behind ---- sofa\\nwall ---- supports ---- ceiling\\nceiling ---- has_on ---- fan\\nfan ---- located_on ---- ceiling\\nfan ---- has_on ---- light_fixture\\nlight_fixture ---- attached_to ---- fan\\nwall ---- has_on ---- outlet\\noutlet ---- located_on ---- wall\\nwall ---- supports ---- ceiling\\nceiling ---- has_on ---- fan\\nfan ---- located_on ---- ceiling\\nceiling ---- has_on ---- fan\\nfan ---- located_on ---- ceiling\\n\\nFrame 000243:\\n\\nman ---- standing_in_front_of ---- sofa\\nsofa ---- positioned_on ---- floor\\nman ---- holding ---- remote_control\\nremote_control ---- in_hand_of ---- man\\nremote_control ---- near ---- sofa\\nsofa ---- positioned_behind ---- television\\nman ---- looking_at ---- remote_control\\nremote_control ---- on_top_of ---- sofa\\nman ---- reaching_towards ---- sofa\\nsofa ---- has ---- throw_pillow\\nman ---- standing_near ---- wall\\nwall ---- has ---- electrical_outlet\\nelectrical_outlet ---- has ---- light_fixture\\nlight_fixture ---- above ---- electrical_outlet\\nsofa ---- positioned_on ---- floor\\nwall ---- has ---- electrical_outlet\\n\\nFrame 000267:\\n\\nman ---- standing_in_front_of ---- sofa\\nsofa ---- positioned_in_front_of ---- wall\\nman ---- holding ---- remote_control\\nremote_control ---- in_hand_of ---- man\\nremote_control ---- pointing_at ---- wall\\nwall ---- positioned_behind ---- sofa\\nwall ---- containing ---- electrical_outlet\\nelectrical_outlet ---- connected_to ---- fan\\nfan ---- mounted_on ---- wall\\nfan ---- rotating_fan_blades\\nwall ---- containing ---- light_switch\\nlight_switch ---- adjacent_to ---- electrical_outlet\\nwall ---- supporting ---- fan\\nfan ---- connected_to ---- electrical_cord\\nelectrical_cord ---- running_to ---- electrical_outlet\\nsofa ---- positioned_above ---- floor\\nfloor ---- covering ---- room\\nwall ---- reflecting ---- light\\n\\nFrame 000274:\\n\\nman ---- standing_in_front_of ---- sofa\\nsofa ---- positioned_in_front_of ---- coffee_table\\ncoffee_table ---- located_in_front_of ---- man\\nlamp ---- placed_next_to ---- coffee_table\\nsofa ---- on ---- wall\\nwall ---- contains ---- electrical_outlet\\nelectrical_outlet ---- adjacent_to ---- light_switch\\nlight_switch ---- mounted_on ---- wall\\nman ---- looking_at ---- sofa\\nsofa ---- has ---- lamp\\nwall ---- supports ---- electrical_outlet\\nman ---- adjacent_to ---- wall\\nsofa ---- has ---- curtain\\nwall ---- contains ---- window\\nwindow ---- has ---- curtain\\n\\nFrame 000294:\\n\\nman ---- holding ---- remote_control\\nman ---- standing_in_front_of ---- sofa\\nsofa ---- positioned_in_front_of ---- wall\\nremote_control ---- held_by ---- man\\nwall ---- behind ---- sofa\\nceiling_fan ---- mounted_on ---- ceiling\\nceiling_fan ---- above ---- sofa\\nwall_light ---- mounted_on ---- wall\\nwall_light ---- to_the_right_of ---- sofa\\nman ---- standing_on ---- floor\\nfloor ---- tiled ---- floor\\nwall ---- behind ---- wall_light\\nman ---- facing ---- ceiling_fan\\nsofa ---- positioned_in_front_of ---- wall\\nfloor ---- tiled ---- floor\\n\\nFrame 000313:\\n\\nman ---- standing_in_front_of ---- chair\\nchair ---- has_on ---- throw_pillow\\nthrow_pillow ---- located_on ---- chair\\nman ---- wearing ---- shirt\\nman ---- wearing ---- shorts\\nman ---- wearing ---- shoes\\nchair ---- located_on ---- rug\\nrug ---- located_on ---- floor\\nfloor ---- located_on ---- wall\\nman ---- looking_at ---- chair\\nchair ---- located_behind ---- wall\\nman ---- standing_on ---- rug\\nrug ---- located_on ---- floor\\nfloor ---- located_on ---- wall\\nman ---- interacting_with ---- chair\\nchair ---- located_on ---- floor\\nfloor ---- located_on ---- wall\\nman ---- standing_on ---- rug\\nrug ---- located_on ---- floor\\nfloor ---- located_on ---- wall\\n\\nFrame 000320:\\n\\nwoman ---- holding ---- spray_bottle\\nspray_bottle ---- containing ---- blue_liquid\\nblue_liquid ---- spraying_on ---- cushion\\ncushion ---- on ---- sofa\\nsofa ---- part_of ---- room\\nwoman ---- standing_in_front_of ---- sofa\\nsofa ---- located_on ---- floor\\nfloor ---- part_of ---- room\\nwoman ---- looking_at ---- cushion\\ncushion ---- on_top_of ---- sofa\\nsofa ---- located_behind ---- wall\\nwoman ---- wearing ---- t_shirt\\nt_shirt ---- part_of ---- woman\\nwoman ---- wearing ---- shorts\\nshorts ---- part_of ---- woman\\nwoman ---- wearing ---- sneakers\\nsneakers ---- part_of ---- woman\\nfloor ---- located_under ---- sofa\\nwall ---- located_behind ---- sofa\\nwindow ---- located_behind ---- sofa\\n\\nFrame 000336:\\n\\nyoung_man ---- standing_in_front_of ---- sofa\\nsofa ---- sitting_on ---- cushion\\ncushion ---- on ---- sofa\\nyoung_man ---- holding ---- cleaning_tool\\ncleaning_tool ---- in_hand_of ---- young_man\\nsofa ---- against ---- wall\\nwall ---- behind ---- sofa\\nwindow ---- in_background ---- scene\\nwindow ---- with ---- window_sill\\nwindow_sill ---- part_of ---- window\\nceiling_fan ---- mounted_on ---- ceiling\\nceiling_fan ---- with ---- light_fixture\\nlight_fixture ---- attached_to ---- ceiling_fan\\nwindow ---- with ---- curtain\\ncurtain ---- hanging_from ---- window\\nyoung_man ---- looking_at ---- sofa\\nsofa ---- against ---- wall\\nceiling_fan ---- above ---- sofa\\nyoung_man ---- standing_in_front_of ---- sofa\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]['stsg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': '    Look carefully at this image and identify all objects and relationships present.\\n\\n    First, list all distinct objects you can detect in the image. Be thorough and specific with your object labels (e.g., \"young woman\" rather than just \"person\", \"wooden chair\" rather than just \"chair\").\\n\\n    Then, describe the key relationships between these objects in free-form text. Consider:\\n    - Spatial relationships (above, below, behind, inside, etc.)\\n    - Action-based relationships (holding, looking at, sitting on, etc.)\\n    - Physical connections (attached to, part of, touching, etc.)\\n    - Relative positions (next to, between, surrounding, etc.)\\n\\n    Think step by step.'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]['chat_history'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = list(frames for frames in main.generate_frames(iters=2))\n",
    "frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data/datasets/STAR/STAR_annotations/STAR_val.json\") as in_file:\n",
    "    star_data = json.load(in_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 222\n",
    "test_sample = star_data[test_id]\n",
    "list(test_sample.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['situations']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = dict()\n",
    "\n",
    "for frame_id, frame_info in test_sample['situations'].items():\n",
    "    graph_dict[frame_id] = []\n",
    "    for i in range(len(frame_info['rel_pairs'])):\n",
    "\n",
    "        assert len(frame_info['rel_pairs']) == len(frame_info['rel_labels'])\n",
    "        \n",
    "        entry = (frame_info['rel_pairs'][i][0], frame_info['rel_labels'][i], frame_info['rel_pairs'][i][1])\n",
    "        graph_dict[frame_id].append(entry)\n",
    "\n",
    "graph_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_Question_Answer_Options(test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['video_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['start'], test_sample['end']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['situations']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_video_dir = pathlib.Path('../data/datasets/action-genome/Charades_v1_480/')\n",
    "raw_frame_dir = pathlib.Path('../data/datasets/action-genome/frames/')\n",
    "save_video_dir = pathlib.Path('/home/lusha/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(raw_frame_dir / f\"{test_sample['video_id']}.mp4\" / f\"000228.png\")\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display using Matplotlib\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_SituationGraph(test_sample, 1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_Video(test_sample, raw_video_dir, save_video_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from STAR_utils.visualization_tools import vis_utils\n",
    "\n",
    "frame_ids = vis_utils.sample_frames(list(test_sample['situations'].keys()), 10)\n",
    "frame_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(vis_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames(frame_list, frame_dir):\n",
    "    select = []\n",
    "    for i in range(len(frame_list)):\n",
    "        frame = cv2.imread(frame_dir / f\"{frame_list[i]}.png\")\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        select.append(frame)\n",
    "        \n",
    "    return select\n",
    "    \n",
    "\n",
    "frame_dir = raw_frame_dir / f\"{test_sample['video_id']}.mp4\"\n",
    "frames = load_frames(frame_ids, frame_dir)\n",
    "\n",
    "print(len(frames))\n",
    "frames[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_Keyframes(test_sample,fps,200,raw_frame_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qaviz.Vis_Keyframes(test_sample,fps,10,raw_frame_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_Keyframes(test_sample,fps,10,raw_frame_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_Keyframes(test_sample,fps,4,raw_frame_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_Keyframes(test_sample,fps, 15, raw_frame_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaviz.Vis_Box(test_sample,fps, 5, raw_frame_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "base64_encodings = []\n",
    "for f_id in frame_ids:\n",
    "    frame_path = raw_frame_dir / f\"{test_sample['video_id']}.mp4\" / f\"{f_id}.png\"\n",
    "    with open(frame_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "        base64_encodings.append(base64.b64encode(img_bytes).decode('utf-8'))\n",
    "        \n",
    "len(base64_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama_manager as om\n",
    "\n",
    "\n",
    "url = os.environ.get(\"OLLAMA_URL\", \"http://lusha_ollama:11435\")\n",
    "\n",
    "ollama_params = {\n",
    "    \"model\": \"gemma3:4b\"\n",
    "}\n",
    "\n",
    "client = om.OllamaRequestManager(url, ollama_params)\n",
    "r = client.ollama_completion_request(\n",
    "    endpoint='generate',\n",
    "    payload={\n",
    "        \"model\": \"gemma3:4b\",\n",
    "        \"prompt\": \"how are you\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"\"\"\\\n",
    "Look carefully at this image and identify all objects and relationships present.\n",
    "\n",
    "First, list all distinct objects you can detect in the image. Be thorough and specific with your object labels (e.g., \"young woman\" rather than just \"person\", \"wooden chair\" rather than just \"chair\").\n",
    "\n",
    "Then, describe the key relationships between these objects in free-form text. Consider:\n",
    "- Spatial relationships (above, below, behind, inside, etc.)\n",
    "- Action-based relationships (holding, looking at, sitting on, etc.)\n",
    "- Physical connections (attached to, part of, touching, etc.)\n",
    "- Relative positions (next to, between, surrounding, etc.)\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "\n",
    "msg1 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt1,\n",
    "}\n",
    "resp = client.ollama_completion_request(\n",
    "            endpoint='chat',\n",
    "            payload={\n",
    "                \"model\": \"gemma3:4b\",\n",
    "                \"messages\": [msg1],\n",
    "                \"options\": {\n",
    "                    \"num_ctx\": 10_000\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "prompt2 = \\\n",
    "    \"\"\"\\\n",
    "    Thank you. Now organize the objects and relationships you identified into a formal scene graph using this format:\n",
    "    object1 ---- relationship ---- object2\n",
    "    \n",
    "    The list of relationship predicates should be introduced by the tag <scene_graph> and terminated by the tag </scene_graph>\n",
    "    For example:\n",
    "    woman ---- sitting_on ---- chair\n",
    "    dog ---- lying_under ---- table\n",
    "    book ---- on_top_of ---- shelf\n",
    "\n",
    "    Please follow these guidelines:\n",
    "    1. Create at least 10 relationship triplets (more if the image is complex)\n",
    "    2. Use specific and consistent object labels\n",
    "    3. Use concise but descriptive relationship terms (connect words with underscores)\n",
    "    4. Include all meaningful relationships between objects\n",
    "    5. Verify that all objects you identified in step 1 appear in at least one relationship\n",
    "\n",
    "    Your scene graph:\\\n",
    "    \"\"\"\n",
    "\n",
    "messages = [msg1, resp, {\"role\": \"user\", \"content\": prompt2}]\n",
    "resp = client.ollama_completion_request(\n",
    "            endpoint='chat',\n",
    "            payload={\n",
    "                \"model\": \"gemma3:4b\",\n",
    "                \"messages\": messages,\n",
    "                \"options\": {\n",
    "                    \"num_ctx\": 10_000\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Okay, here’s the scene graph representation of the image, following your specified format and guidelines:\\n\\n<scene_graph>\\nman ---- standing_in_front_of ---- sofa\\nsofa ---- positioned_in_front_of ---- coffee_table\\ncoffee_table ---- covered_by ---- tablecloth\\ntablecloth ---- on ---- coffee_table\\nman ---- looking_at ---- coffee_table\\nlamp ---- on ---- metal_stand\\nmetal_stand ---- supporting ---- lamp\\nlamp ---- positioned_on ---- coffee_table\\nwindow ---- containing ---- curtain\\ncurtain ---- hanging_in ---- window\\nwindow ---- mounted_on ---- wall\\nwall ---- supporting ---- window\\nman ---- facing ---- coffee_table\\nsofa ---- adjacent_to ---- coffee_table\\n</scene_graph>\\n\\nI’ve aimed for clarity and accuracy in describing the relationships. Let me know if you’d like me to refine any of the triplets or add more detail!\"\n",
    "\n",
    "# the ?s: specify the re.DOTALL flag\n",
    "pattern = \"(?<=<scene_graph>)(?s:.+)(?=</scene_graph)\"\n",
    "res = re.search(pattern,test_str)\n",
    "if res:\n",
    "    print(res.group(0))\n",
    "    \n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
